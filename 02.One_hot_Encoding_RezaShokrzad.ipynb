{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "One-hot Encoding - RezaShokrzad.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#One-hot Encoding\n"
      ],
      "metadata": {
        "id": "1JSpkx1LjQFL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##In Numpy\n",
        "Step1. Convert Text to lower case \\\n",
        "Step2. Tokenize the text \\\n",
        "Step3. Get unique words \\\n",
        "Step4. Sort the word list \\\n",
        "Step5. Get the integer/position of the words \\\n",
        "Step6. create a vector of each word by marking its position as 1 and rest as 0 \\\n",
        "Step7. create a matrix of the found vectors."
      ],
      "metadata": {
        "id": "fm6ZHAbbjVEq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxW5g7x5jBw8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9403296c-cf4d-4047-ad09-81fd3b3a4747"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "values:  ['i', 'learn', 'nlp', 'should']\n",
            "\n",
            "integer encoded:  [3, 0, 1, 2]\n",
            "\n",
            "MATRIX:\n",
            "[[0 0 0 1]\n",
            " [1 0 0 0]\n",
            " [0 1 0 0]\n",
            " [0 0 1 0]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "docs = \"Should I learn NLP\".lower().split()\n",
        "doc1 = set(docs)\n",
        "doc1 = sorted(doc1)\n",
        "print (\"\\nvalues: \", doc1)\n",
        "\n",
        "integer_encoded = []\n",
        "for i in docs:\n",
        "    v = np.where( np.array(doc1) == i)[0][0]\n",
        "    integer_encoded.append(v)\n",
        "print (\"\\ninteger encoded: \",integer_encoded)\n",
        "\n",
        "def get_vec(len_doc,word):\n",
        "    empty_vector = [0] * len_doc\n",
        "    vect = 0\n",
        "    find = np.where( np.array(doc1) == word)[0][0]\n",
        "    empty_vector[find] = 1\n",
        "    return empty_vector\n",
        "\n",
        "def get_matrix(doc1):\n",
        "    mat = []\n",
        "    len_doc = len(doc1)\n",
        "    for i in docs:\n",
        "        vec = get_vec(len_doc,i)\n",
        "        mat.append(vec)\n",
        "        \n",
        "    return np.asarray(mat)\n",
        "\n",
        "print (\"\\nMATRIX:\")\n",
        "print (get_matrix(doc1))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##In Sklearn\n",
        "\n",
        "Step1. Convert the text to Lower Case \\\n",
        "Step2. Word Tokenize \\\n",
        "Step3. Get its integer value i.e the position by using **LabelEncoder()** \\\n",
        "Step4. Get one hot encoding of the word by referring to the label encoded values using **OneHotEncoder()**"
      ],
      "metadata": {
        "id": "z227l04WkKm5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "# define example\n",
        "# data = ['cold', 'cold', 'warm', 'cold', 'hot', 'hot', 'warm', 'cold', 'warm', 'hot']\n",
        "\n",
        "\n",
        "doc1 = \"Should I learn NLP\".lower()\n",
        "doc2 = \"You must learn NLP\".lower()\n",
        "doc1 = doc1.split()\n",
        "doc2 = doc2.split()\n",
        "doc1_array = array(doc1)\n",
        "doc2_array = array(doc2)\n",
        "doc3 = doc1+doc2\n",
        "# doc3 = set(doc3)\n",
        "data = list(doc3)\n",
        "\n",
        "\n",
        "values = array(data)\n",
        "print(values)\n",
        "# integer encode\n",
        "label_encoder = LabelEncoder()\n",
        "integer_encoded = label_encoder.fit_transform(values)\n",
        "print(integer_encoded)\n",
        "\n",
        "# binary encode\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
        "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
        "print(onehot_encoded)\n",
        "\n",
        "\n",
        "# invert first example\n",
        "inverted = label_encoder.inverse_transform([argmax(onehot_encoded[0, :])])\n",
        "print(inverted)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4YrkkYRkC-O",
        "outputId": "6f876db7-f8d2-40bc-d413-d7a76fef076c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['should' 'i' 'learn' 'nlp' 'you' 'must' 'learn' 'nlp']\n",
            "[4 0 1 3 5 2 1 3]\n",
            "[[0. 0. 0. 0. 1. 0.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]]\n",
            "['should']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##In Keras\n",
        "\n",
        "Step1. Convert the text to Lower Case \\\n",
        "Step2. Word Tokenize \\\n",
        "Step3. Get its integer value i.e the position by using **LabelEncoder()** \\\n",
        "Step4. Get one hot encoding of the word by referring to the label encoded values by using **to_categorical()**"
      ],
      "metadata": {
        "id": "KxBq3Dk-keGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "\n",
        "doc = \"Should I learn NLP\".lower().split()\n",
        "\n",
        "def using_Tokenizer(doc):\n",
        "    # create the tokenizer\n",
        "    t = Tokenizer()\n",
        "    # fit the tokenizer on the documents\n",
        "    t.fit_on_texts(doc)\n",
        "\n",
        "    # integer encode documents\n",
        "    encoded_docs = t.texts_to_matrix(doc, mode='count')\n",
        "    return encoded_docs\n",
        "\n",
        "def using_to_categorical(doc):\n",
        "    label_encoder = LabelEncoder()\n",
        "    data = label_encoder.fit_transform(doc)\n",
        "    data = array(data)\n",
        "\n",
        "    # one hot encode\n",
        "    encoded = to_categorical(data)\n",
        "    return encoded\n",
        "\n",
        "def invert_encoding(row_num):\n",
        "    inverted = label_encoder.inverse_transform([argmax(onehot_encoded[row_num, :])])\n",
        "    return inverted\n",
        "    \n",
        "print (\"===using Keras Tokenizer for OneHotEncoding===\")\n",
        "print (using_Tokenizer(doc))\n",
        "print ()\n",
        "print (\"===using Keras to_categorical for OneHotEncoding===\")\n",
        "print (using_to_categorical(doc))\n",
        "print ()\n",
        "print (invert_encoding(int(0)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHF3Z6Vhkbsc",
        "outputId": "2aa9b01d-48eb-454f-9410-58a99e6e1ea8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===using Keras Tokenizer for OneHotEncoding===\n",
            "[[0. 1. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 1.]]\n",
            "\n",
            "===using Keras to_categorical for OneHotEncoding===\n",
            "[[0. 0. 0. 1.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 1. 0.]]\n",
            "\n",
            "['should']\n"
          ]
        }
      ]
    }
  ]
}